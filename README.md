# No-country-for-old-men
this is repo for the project code of information retrieval.

## No Country for Old Men (P4)
Today the debate about the risks associated with the pervasive use of artificial intelligence in human communication is of great importance. In particular, Large Language Models are at the center of these concerns as the use of these technologies can potentially lead to the spread and increase of disinformation, erroneous beliefs, social bias and the spread of stereotypes that can affect the ethical dimension of communication.

In this framework, the project aims to propose a methodology and techniques to test existing pre-trained models and measure the ethical risk they may entail. In particular, you are asked to:

Choose a model and a task (e.g., text classification, question answering, data summarization, text generation, machine translation, word embedding).

Precisely define a measurable research question regarding a possible discriminatory behavior of the model (e.g., is there a gender bias in predicting occupation? Is there a stereotype regarding ethnicity with respect to deviant social behavior ?). Students are encouraged to expand the list beyond examples, but clearly define the goal.

Define a test method with which to evaluate the behavior of the chosen model. Produce measurable results that allow to estimate the risk associated with the chosen pre-trained model.

<strong>Dataset</strong>

Any pre-trained model from https://huggingface.co/ or from any other source

<strong>References</strong>

1. Garg, N., Schiebinger, L., Jurafsky, D., & Zou, J. (2018). Word embeddings quantify 100 years of gender and ethnic stereotypes. Proceedings of the National Academy of Sciences, 115(16), E3635-E3644.
2. Bolukbasi, T., Chang, K. W., Zou, J. Y., Saligrama, V., & Kalai, A. T. (2016). Man is to computer programmer as woman is to homemaker? debiasing word embeddings. Advances in neural information processing systems, 29.
3. Kiritchenko, S., & Mohammad, S. M. (2018). Examining gender and race bias in two hundred sentiment analysis systems. arXiv preprint arXiv:1805.04508.
4. Dixon, L., Li, J., Sorensen, J., Thain, N., & Vasserman, L. (2018, December). Measuring and mitigating unintended bias in text classification. In Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society (pp. 67-73).
